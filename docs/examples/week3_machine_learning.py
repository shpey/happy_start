#!/usr/bin/env python3
"""
æ™ºèƒ½æ€ç»´é¡¹ç›® - ç¬¬ä¸‰å‘¨æœºå™¨å­¦ä¹ åŸºç¡€ç¤ºä¾‹
è¿™ä¸ªæ–‡ä»¶åŒ…å«äº†Scikit-learnæœºå™¨å­¦ä¹ çš„å®è·µç¤ºä¾‹
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# ==================== ç›‘ç£å­¦ä¹ ï¼šåˆ†ç±»ä»»åŠ¡ ====================

def thinking_style_classification():
    """æ€ç»´é£æ ¼åˆ†ç±»ä»»åŠ¡ - æ ¹æ®è®¤çŸ¥ç‰¹å¾é¢„æµ‹å­¦ä¹ é£æ ¼"""
    print("ğŸ¯ æ€ç»´é£æ ¼åˆ†ç±»ä»»åŠ¡")
    print("-" * 40)
    
    # åŠ è½½æ•°æ®
    df = pd.read_csv('data/thinking_dataset.csv')
    
    # ç‰¹å¾é€‰æ‹©ï¼šè®¤çŸ¥ç›¸å…³ç‰¹å¾
    feature_cols = ['iq_score', 'creativity_score', 'logic_score', 
                   'emotional_intelligence', 'problem_solving_time', 'accuracy_rate']
    X = df[feature_cols]
    y = df['learning_style']
    
    # æ•°æ®é¢„å¤„ç†
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # ç¼–ç æ ‡ç­¾
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(X_test)}")
    print(f"å­¦ä¹ é£æ ¼ç±»åˆ«: {label_encoder.classes_}")
    
    # å°è¯•å¤šç§åˆ†ç±»ç®—æ³•
    classifiers = {
        'éšæœºæ£®æ—': RandomForestClassifier(n_estimators=100, random_state=42),
        'é€»è¾‘å›å½’': LogisticRegression(random_state=42, max_iter=1000),
        'æ”¯æŒå‘é‡æœº': SVC(random_state=42),
        'Kè¿‘é‚»': KNeighborsClassifier(n_neighbors=5)
    }
    
    results = {}
    
    for name, clf in classifiers.items():
        # è®­ç»ƒæ¨¡å‹
        clf.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred = clf.predict(X_test)
        
        # è¯„ä¼°
        accuracy = accuracy_score(y_test, y_pred)
        
        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(clf, X_scaled, y_encoded, cv=5)
        
        results[name] = {
            'accuracy': accuracy,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std()
        }
        
        print(f"\n{name}:")
        print(f"  æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.3f}")
        print(f"  äº¤å‰éªŒè¯: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")
    
    # é€‰æ‹©æœ€ä½³æ¨¡å‹
    best_model_name = max(results.keys(), key=lambda k: results[k]['accuracy'])
    best_model = classifiers[best_model_name]
    
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
    
    # ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆå¦‚æœæ”¯æŒï¼‰
    if hasattr(best_model, 'feature_importances_'):
        feature_importance = pd.DataFrame({
            'feature': feature_cols,
            'importance': best_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("\nç‰¹å¾é‡è¦æ€§:")
        print(feature_importance)
    
    return best_model, scaler, label_encoder

# ==================== ç›‘ç£å­¦ä¹ ï¼šå›å½’ä»»åŠ¡ ====================

def thinking_capacity_prediction():
    """æ€ç»´èƒ½åŠ›é¢„æµ‹ä»»åŠ¡ - é¢„æµ‹ç»¼åˆæ€ç»´èƒ½åŠ›æŒ‡æ•°"""
    print("\nğŸ“ˆ æ€ç»´èƒ½åŠ›é¢„æµ‹ä»»åŠ¡")
    print("-" * 40)
    
    # åŠ è½½æ•°æ®
    df = pd.read_csv('data/thinking_dataset.csv')
    
    # ç‰¹å¾é€‰æ‹©
    feature_cols = ['age', 'iq_score', 'creativity_score', 'logic_score', 
                   'emotional_intelligence', 'problem_solving_time', 'accuracy_rate']
    X = df[feature_cols]
    y = df['thinking_capacity_index']
    
    # æ•°æ®é¢„å¤„ç†
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42
    )
    
    # å°è¯•å¤šç§å›å½’ç®—æ³•
    regressors = {
        'éšæœºæ£®æ—å›å½’': RandomForestRegressor(n_estimators=100, random_state=42),
        'çº¿æ€§å›å½’': LinearRegression()
    }
    
    results = {}
    
    for name, reg in regressors.items():
        # è®­ç»ƒæ¨¡å‹
        reg.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred = reg.predict(X_test)
        
        # è¯„ä¼°
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        results[name] = {
            'mse': mse,
            'r2': r2,
            'rmse': np.sqrt(mse)
        }
        
        print(f"\n{name}:")
        print(f"  å‡æ–¹è¯¯å·® (MSE): {mse:.6f}")
        print(f"  RÂ² åˆ†æ•°: {r2:.3f}")
        print(f"  å‡æ–¹æ ¹è¯¯å·® (RMSE): {np.sqrt(mse):.6f}")
    
    # é€‰æ‹©æœ€ä½³æ¨¡å‹
    best_model_name = max(results.keys(), key=lambda k: results[k]['r2'])
    best_model = regressors[best_model_name]
    
    print(f"\nğŸ† æœ€ä½³å›å½’æ¨¡å‹: {best_model_name}")
    
    return best_model, scaler

# ==================== æ— ç›‘ç£å­¦ä¹ ï¼šèšç±»åˆ†æ ====================

def thinking_pattern_clustering():
    """æ€ç»´æ¨¡å¼èšç±»åˆ†æ"""
    print("\nğŸ” æ€ç»´æ¨¡å¼èšç±»åˆ†æ")
    print("-" * 40)
    
    # åŠ è½½æ•°æ®
    df = pd.read_csv('data/thinking_dataset.csv')
    
    # é€‰æ‹©è®¤çŸ¥ç‰¹å¾
    feature_cols = ['iq_score', 'creativity_score', 'logic_score', 'emotional_intelligence']
    X = df[feature_cols]
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # ç¡®å®šæœ€ä½³èšç±»æ•°é‡
    inertias = []
    K_range = range(2, 9)
    
    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X_scaled)
        inertias.append(kmeans.inertia_)
    
    # ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™é€‰æ‹©Kå€¼
    # ç®€å•é€‰æ‹©K=4 (å¯¹åº”ä¸åŒçš„æ€ç»´æ¨¡å¼)
    optimal_k = 4
    
    print(f"é€‰æ‹©èšç±»æ•°é‡: {optimal_k}")
    
    # æ‰§è¡ŒK-meansèšç±»
    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X_scaled)
    
    # æ·»åŠ èšç±»æ ‡ç­¾åˆ°æ•°æ®æ¡†
    df_clustered = df.copy()
    df_clustered['cluster'] = cluster_labels
    
    # åˆ†æå„èšç±»çš„ç‰¹å¾
    print("\nèšç±»åˆ†æç»“æœ:")
    cluster_analysis = df_clustered.groupby('cluster')[feature_cols].mean()
    print(cluster_analysis.round(2))
    
    # ä¸ºæ¯ä¸ªèšç±»å‘½å
    cluster_names = {
        0: "å¹³è¡¡å‹æ€ç»´è€…",
        1: "é€»è¾‘ä¸»å¯¼å‹",
        2: "åˆ›æ„å‹æ€ç»´è€…", 
        3: "æƒ…æ„Ÿæ™ºèƒ½å‹"
    }
    
    print(f"\nèšç±»è§£é‡Š:")
    for cluster_id, cluster_name in cluster_names.items():
        cluster_data = cluster_analysis.loc[cluster_id]
        dominant_feature = cluster_data.idxmax()
        print(f"èšç±» {cluster_id} - {cluster_name}:")
        print(f"  ä¸»å¯¼ç‰¹å¾: {dominant_feature}")
        print(f"  æ ·æœ¬æ•°é‡: {sum(cluster_labels == cluster_id)}")
    
    return kmeans, scaler, cluster_names

# ==================== æ— ç›‘ç£å­¦ä¹ ï¼šé™ç»´åˆ†æ ====================

def thinking_dimension_reduction():
    """æ€ç»´ç‰¹å¾é™ç»´åˆ†æ"""
    print("\nğŸ“Š æ€ç»´ç‰¹å¾é™ç»´åˆ†æ (PCA)")
    print("-" * 40)
    
    # åŠ è½½æ•°æ®
    df = pd.read_csv('data/thinking_dataset.csv')
    
    # é€‰æ‹©æ•°å€¼ç‰¹å¾
    numeric_features = ['age', 'iq_score', 'creativity_score', 'logic_score', 
                       'emotional_intelligence', 'problem_solving_time', 'accuracy_rate']
    X = df[numeric_features]
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # æ‰§è¡ŒPCA
    pca = PCA()
    X_pca = pca.fit_transform(X_scaled)
    
    # åˆ†æä¸»æˆåˆ†
    print("ä¸»æˆåˆ†åˆ†æç»“æœ:")
    print(f"å„ä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹:")
    for i, ratio in enumerate(pca.explained_variance_ratio_):
        print(f"  PC{i+1}: {ratio:.3f} ({ratio*100:.1f}%)")
    
    # ç´¯ç§¯è§£é‡Šæ–¹å·®
    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)
    print(f"\nå‰3ä¸ªä¸»æˆåˆ†ç´¯ç§¯è§£é‡Šæ–¹å·®: {cumulative_variance[2]:.3f} ({cumulative_variance[2]*100:.1f}%)")
    
    # é€‰æ‹©ä¿ç•™çš„ä¸»æˆåˆ†æ•°é‡ï¼ˆè§£é‡Š95%æ–¹å·®ï¼‰
    n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1
    print(f"è§£é‡Š95%æ–¹å·®éœ€è¦çš„ä¸»æˆåˆ†æ•°é‡: {n_components_95}")
    
    # é‡æ–°æ‰§è¡ŒPCAï¼Œä¿ç•™ä¸»è¦æˆåˆ†
    pca_reduced = PCA(n_components=n_components_95)
    X_pca_reduced = pca_reduced.fit_transform(X_scaled)
    
    # åˆ†æä¸»æˆåˆ†ç»„æˆ
    print(f"\nä¸»æˆåˆ†ç»„æˆ (å‰{n_components_95}ä¸ª):")
    components_df = pd.DataFrame(
        pca_reduced.components_[:n_components_95].T,
        columns=[f'PC{i+1}' for i in range(n_components_95)],
        index=numeric_features
    )
    print(components_df.round(3))
    
    return pca_reduced, scaler

# ==================== æ¨¡å‹é›†æˆä¸è¯„ä¼° ====================

def create_thinking_ai_system():
    """åˆ›å»ºæ™ºèƒ½æ€ç»´AIç³»ç»Ÿ"""
    print("\nğŸ¤– åˆ›å»ºæ™ºèƒ½æ€ç»´AIç³»ç»Ÿ")
    print("-" * 40)
    
    # è®­ç»ƒå„ç§æ¨¡å‹
    print("1. è®­ç»ƒæ€ç»´é£æ ¼åˆ†ç±»å™¨...")
    style_classifier, style_scaler, style_encoder = thinking_style_classification()
    
    print("\n2. è®­ç»ƒæ€ç»´èƒ½åŠ›é¢„æµ‹å™¨...")
    capacity_predictor, capacity_scaler = thinking_capacity_prediction()
    
    print("\n3. è®­ç»ƒæ€ç»´æ¨¡å¼èšç±»å™¨...")
    pattern_clusterer, cluster_scaler, cluster_names = thinking_pattern_clustering()
    
    print("\n4. è®­ç»ƒé™ç»´åˆ†æå™¨...")
    dimension_reducer, dim_scaler = thinking_dimension_reduction()
    
    # åˆ›å»ºç»¼åˆAIç³»ç»Ÿç±»
    class ThinkingAISystem:
        def __init__(self, style_clf, capacity_pred, pattern_cluster, dim_reducer, 
                     style_scaler, capacity_scaler, cluster_scaler, dim_scaler, style_encoder, cluster_names):
            self.style_classifier = style_clf
            self.capacity_predictor = capacity_pred
            self.pattern_clusterer = pattern_cluster
            self.dimension_reducer = dim_reducer
            self.style_scaler = style_scaler
            self.capacity_scaler = capacity_scaler
            self.cluster_scaler = cluster_scaler
            self.dim_scaler = dim_scaler
            self.style_encoder = style_encoder
            self.cluster_names = cluster_names
        
        def analyze_user(self, user_data):
            """åˆ†æç”¨æˆ·çš„æ€ç»´ç‰¹å¾"""
            # æå–ç‰¹å¾
            style_features = [user_data['iq_score'], user_data['creativity_score'], user_data['logic_score'], 
                             user_data['emotional_intelligence'], user_data['problem_solving_time'], user_data['accuracy_rate']]
            capacity_features = [user_data['age'], user_data['iq_score'], user_data['creativity_score'], user_data['logic_score'], 
                                user_data['emotional_intelligence'], user_data['problem_solving_time'], user_data['accuracy_rate']]
            cluster_features = [user_data['iq_score'], user_data['creativity_score'], user_data['logic_score'], user_data['emotional_intelligence']]
            
            # é¢„æµ‹å­¦ä¹ é£æ ¼
            style_scaled = self.style_scaler.transform([style_features])
            style_pred = self.style_classifier.predict(style_scaled)[0]
            predicted_style = self.style_encoder.inverse_transform([style_pred])[0]
            
            # é¢„æµ‹æ€ç»´èƒ½åŠ› (ä½¿ç”¨æ­£ç¡®çš„scaler)
            capacity_scaled = self.capacity_scaler.transform([capacity_features])
            predicted_capacity = self.capacity_predictor.predict(capacity_scaled)[0]
            
            # èšç±»åˆ†æ
            cluster_scaled = self.cluster_scaler.transform([cluster_features])
            cluster_pred = self.pattern_clusterer.predict(cluster_scaled)[0]
            thinking_pattern = self.cluster_names[cluster_pred]
            
            return {
                'learning_style': predicted_style,
                'thinking_capacity': predicted_capacity,
                'thinking_pattern': thinking_pattern,
                'recommendations': self._generate_recommendations(predicted_style, thinking_pattern)
            }
        
        def _generate_recommendations(self, style, pattern):
            """æ ¹æ®åˆ†æç»“æœç”Ÿæˆå»ºè®®"""
            recommendations = []
            
            if style == 'visual':
                recommendations.append("ä½¿ç”¨å›¾è¡¨ã€æ€ç»´å¯¼å›¾ç­‰è§†è§‰åŒ–å·¥å…·")
            elif style == 'auditory':
                recommendations.append("é€šè¿‡è®²è§£ã€è®¨è®ºç­‰å¬è§‰æ–¹å¼å­¦ä¹ ")
            elif style == 'kinesthetic':
                recommendations.append("é€šè¿‡å®è·µæ“ä½œã€ä½“éªŒå¼å­¦ä¹ ")
            elif style == 'reading':
                recommendations.append("é€šè¿‡é˜…è¯»æ–‡æœ¬ã€ç¬”è®°ç­‰æ–¹å¼å­¦ä¹ ")
            
            if 'åˆ›æ„' in pattern:
                recommendations.append("å¤šå‚ä¸å¤´è„‘é£æš´å’Œåˆ›æ–°é¡¹ç›®")
            elif 'é€»è¾‘' in pattern:
                recommendations.append("åŠ å¼ºé€»è¾‘æ¨ç†å’Œåˆ†æè®­ç»ƒ")
            elif 'æƒ…æ„Ÿ' in pattern:
                recommendations.append("æ³¨é‡å›¢é˜Ÿåä½œå’Œæƒ…æ„Ÿæ™ºèƒ½å‘å±•")
            
            return recommendations
    
    # åˆ›å»ºç³»ç»Ÿå®ä¾‹
    ai_system = ThinkingAISystem(
        style_classifier, capacity_predictor, pattern_clusterer, dimension_reducer,
        style_scaler, capacity_scaler, cluster_scaler, dim_scaler, style_encoder, cluster_names
    )
    
    print("\nâœ… æ™ºèƒ½æ€ç»´AIç³»ç»Ÿåˆ›å»ºå®Œæˆï¼")
    
    return ai_system

# ==================== ä¸»ç¨‹åº ====================

def main():
    """ä¸»ç¨‹åº"""
    print("ğŸ§  æ™ºèƒ½æ€ç»´é¡¹ç›® - ç¬¬ä¸‰å‘¨æœºå™¨å­¦ä¹ åŸºç¡€")
    print("=" * 60)
    
    # åˆ›å»ºæ™ºèƒ½æ€ç»´AIç³»ç»Ÿ
    ai_system = create_thinking_ai_system()
    
    # æµ‹è¯•ç³»ç»Ÿ
    print("\nğŸ§ª æµ‹è¯•æ™ºèƒ½æ€ç»´AIç³»ç»Ÿ")
    print("-" * 40)
    
    # æ¨¡æ‹Ÿç”¨æˆ·æ•°æ®
    test_user = {
        'age': 25,
        'iq_score': 120,
        'creativity_score': 8.5,
        'logic_score': 7.2,
        'emotional_intelligence': 8.0,
        'problem_solving_time': 25.0,
        'accuracy_rate': 0.85
    }
    
    # åˆ†æç”¨æˆ·
    analysis_result = ai_system.analyze_user(test_user)
    
    print("æµ‹è¯•ç”¨æˆ·åˆ†æç»“æœ:")
    print(f"  å­¦ä¹ é£æ ¼: {analysis_result['learning_style']}")
    print(f"  æ€ç»´èƒ½åŠ›æŒ‡æ•°: {analysis_result['thinking_capacity']:.3f}")
    print(f"  æ€ç»´æ¨¡å¼: {analysis_result['thinking_pattern']}")
    print("  ä¸ªæ€§åŒ–å»ºè®®:")
    for recommendation in analysis_result['recommendations']:
        print(f"    - {recommendation}")
    
    print("\nğŸ‰ ç¬¬ä¸‰å‘¨æœºå™¨å­¦ä¹ åŸºç¡€å­¦ä¹ å®Œæˆï¼")
    print("ğŸ“š ä¸‹ä¸€æ­¥: æ·±åº¦å­¦ä¹ å…¥é—¨ï¼ˆç¬¬5-6å‘¨ï¼‰")
    print("ğŸ’¡ å·²æŒæ¡æŠ€èƒ½: åˆ†ç±»ã€å›å½’ã€èšç±»ã€é™ç»´ã€æ¨¡å‹è¯„ä¼°")

# ==================== ç»ƒä¹ é¢˜ ====================

def practice_exercises():
    """ç¬¬ä¸‰å‘¨ç»ƒä¹ é¢˜"""
    print("\nğŸ¯ ç¬¬ä¸‰å‘¨ç»ƒä¹ é¢˜:")
    print("-" * 30)
    
    exercises = [
        "ç»ƒä¹ 1: å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„æ€ç»´é£æ ¼åˆ†ç±»å™¨ï¼ˆä¸ä½¿ç”¨Scikit-learnï¼‰",
        "ç»ƒä¹ 2: ä½¿ç”¨ç½‘æ ¼æœç´¢ä¼˜åŒ–æ¨¡å‹è¶…å‚æ•°",
        "ç»ƒä¹ 3: å®ç°æ¨¡å‹çš„ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½",
        "ç»ƒä¹ 4: æ·»åŠ æ›´å¤šè¯„ä¼°æŒ‡æ ‡ï¼ˆç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ï¼‰",
        "ç»ƒä¹ 5: åˆ›å»ºä¸€ä¸ªWebç•Œé¢æ¥æµ‹è¯•æ€ç»´åˆ†æç³»ç»Ÿ"
    ]
    
    for exercise in exercises:
        print(exercise)
    
    print("\nğŸ’¡ æç¤º: è¿™äº›ç»ƒä¹ å°†å¸®åŠ©ä½ æ·±å…¥ç†è§£æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µï¼")

if __name__ == "__main__":
    main()
    practice_exercises() 