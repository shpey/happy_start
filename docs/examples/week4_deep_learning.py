#!/usr/bin/env python3
"""
æ™ºèƒ½æ€ç»´é¡¹ç›® - ç¬¬å››å‘¨æ·±åº¦å­¦ä¹ åŸºç¡€ç¤ºä¾‹
è¿™ä¸ªæ–‡ä»¶åŒ…å«äº†PyTorchæ·±åº¦å­¦ä¹ çš„å®è·µç¤ºä¾‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset, TensorDataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)

# ==================== PyTorchåŸºç¡€æ“ä½œ ====================

def pytorch_basics_demo():
    """PyTorchåŸºç¡€æ“ä½œæ¼”ç¤º"""
    print("ğŸ”¥ PyTorchåŸºç¡€æ“ä½œæ¼”ç¤º")
    print("-" * 40)
    
    # 1. å¼ é‡åˆ›å»ºå’Œæ“ä½œ
    print("1. å¼ é‡åŸºç¡€æ“ä½œ:")
    
    # åˆ›å»ºå¼ é‡
    x = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)
    y = torch.randn(2, 3)
    z = torch.zeros(3, 3)
    
    print(f"ä¸€ç»´å¼ é‡: {x}")
    print(f"éšæœºå¼ é‡: \n{y}")
    print(f"é›¶å¼ é‡: \n{z}")
    
    # å¼ é‡è¿ç®—
    print("\n2. å¼ é‡è¿ç®—:")
    a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)
    b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)
    
    print(f"å¼ é‡a: \n{a}")
    print(f"å¼ é‡b: \n{b}")
    print(f"çŸ©é˜µä¹˜æ³•: \n{torch.mm(a, b)}")
    print(f"å…ƒç´ ç›¸ä¹˜: \n{a * b}")
    
    # 3. è‡ªåŠ¨æ¢¯åº¦è®¡ç®—
    print("\n3. è‡ªåŠ¨æ¢¯åº¦è®¡ç®—:")
    x = torch.tensor([2.0], requires_grad=True)
    y = x ** 2 + 3 * x + 1
    
    print(f"x = {x.item()}")
    print(f"y = xÂ² + 3x + 1 = {y.item()}")
    
    # åå‘ä¼ æ’­
    y.backward()
    print(f"dy/dx = 2x + 3 = {x.grad.item()}")
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    print(f"\n4. è®¾å¤‡ä¿¡æ¯:")
    print(f"GPUå¯ç”¨: {torch.cuda.is_available()}")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    return device

# ==================== ç®€å•ç¥ç»ç½‘ç»œ ====================

class SimpleThinkingNet(nn.Module):
    """ç®€å•çš„æ€ç»´åˆ†æç¥ç»ç½‘ç»œ"""
    
    def __init__(self, input_size, hidden_size, num_classes):
        super(SimpleThinkingNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, num_classes)
        self.dropout = nn.Dropout(0.2)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x

def train_thinking_classifier():
    """è®­ç»ƒæ€ç»´é£æ ¼åˆ†ç±»å™¨"""
    print("\nğŸ§  è®­ç»ƒæ€ç»´é£æ ¼ç¥ç»ç½‘ç»œåˆ†ç±»å™¨")
    print("-" * 40)
    
    # åŠ è½½æ•°æ®
    df = pd.read_csv('data/thinking_dataset.csv')
    
    # ç‰¹å¾é€‰æ‹©
    feature_cols = ['iq_score', 'creativity_score', 'logic_score', 
                   'emotional_intelligence', 'problem_solving_time', 'accuracy_rate']
    X = df[feature_cols].values
    y = df['learning_style'].values
    
    # æ•°æ®é¢„å¤„ç†
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # ç¼–ç æ ‡ç­¾
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_tensor = torch.FloatTensor(X_scaled)
    y_tensor = torch.LongTensor(y_encoded)
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X_tensor, y_tensor, test_size=0.2, random_state=42
    )
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(X_test)}")
    print(f"ç‰¹å¾æ•°é‡: {X_train.shape[1]}")
    print(f"ç±»åˆ«æ•°é‡: {len(label_encoder.classes_)}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(X_train, y_train)
    test_dataset = TensorDataset(X_test, y_test)
    
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = SimpleThinkingNet(
        input_size=X_train.shape[1], 
        hidden_size=64, 
        num_classes=len(label_encoder.classes_)
    ).to(device)
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # è®­ç»ƒæ¨¡å‹
    print("\nå¼€å§‹è®­ç»ƒ...")
    num_epochs = 100
    train_losses = []
    train_accuracies = []
    
    model.train()
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        correct = 0
        total = 0
        
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # ç»Ÿè®¡
            epoch_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
        
        # è®°å½•è®­ç»ƒæŒ‡æ ‡
        avg_loss = epoch_loss / len(train_loader)
        accuracy = 100 * correct / total
        train_losses.append(avg_loss)
        train_accuracies.append(accuracy)
        
        if (epoch + 1) % 20 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')
    
    # æµ‹è¯•æ¨¡å‹
    print("\næµ‹è¯•æ¨¡å‹...")
    model.eval()
    with torch.no_grad():
        correct = 0
        total = 0
        all_predicted = []
        all_labels = []
        
        for batch_X, batch_y in test_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            outputs = model(batch_X)
            _, predicted = torch.max(outputs, 1)
            
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
            
            all_predicted.extend(predicted.cpu().numpy())
            all_labels.extend(batch_y.cpu().numpy())
        
        test_accuracy = 100 * correct / total
        print(f'æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.2f}%')
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_labels, all_predicted, 
                              target_names=label_encoder.classes_))
    
    return model, scaler, label_encoder, train_losses, train_accuracies

# ==================== å·ç§¯ç¥ç»ç½‘ç»œ (CNN) ====================

class ThinkingImageCNN(nn.Module):
    """ç”¨äºæ€ç»´å›¾åƒåˆ†æçš„CNN"""
    
    def __init__(self, num_classes=10):
        super(ThinkingImageCNN, self).__init__()
        
        # å·ç§¯å±‚
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        
        # æ± åŒ–å±‚
        self.pool = nn.MaxPool2d(2, 2)
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(64 * 4 * 4, 512)  # å‡è®¾è¾“å…¥æ˜¯32x32
        self.fc2 = nn.Linear(512, 128)
        self.fc3 = nn.Linear(128, num_classes)
        
        # Dropout
        self.dropout = nn.Dropout(0.5)
        
    def forward(self, x):
        # å·ç§¯ + æ¿€æ´» + æ± åŒ–
        x = self.pool(F.relu(self.conv1(x)))  # 32x32 -> 16x16
        x = self.pool(F.relu(self.conv2(x)))  # 16x16 -> 8x8  
        x = self.pool(F.relu(self.conv3(x)))  # 8x8 -> 4x4
        
        # å±•å¹³
        x = x.view(-1, 64 * 4 * 4)
        
        # å…¨è¿æ¥å±‚
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        
        return x

def create_synthetic_image_data():
    """åˆ›å»ºåˆæˆçš„æ€ç»´å›¾åƒæ•°æ®"""
    print("\nğŸ–¼ï¸ åˆ›å»ºæ€ç»´æ¨¡å¼å›¾åƒæ•°æ®é›†")
    print("-" * 40)
    
    # ç”Ÿæˆåˆæˆçš„æ€ç»´æ¨¡å¼å›¾åƒ
    # æ¨¡æ‹Ÿä¸åŒçš„æ€ç»´æ¨¡å¼ï¼šçº¿æ€§ã€èºæ—‹ã€ç½‘çŠ¶ã€éšæœº
    np.random.seed(42)
    
    def generate_pattern_image(pattern_type, size=32):
        """ç”Ÿæˆç‰¹å®šæ¨¡å¼çš„å›¾åƒ"""
        img = np.zeros((size, size))
        
        if pattern_type == 0:  # çº¿æ€§æ¨¡å¼ (é€»è¾‘æ€ç»´)
            for i in range(0, size, 4):
                img[i:i+2, :] = 1
                img[:, i:i+2] = 0.5
                
        elif pattern_type == 1:  # èºæ—‹æ¨¡å¼ (åˆ›æ„æ€ç»´)
            center = size // 2
            for i in range(size):
                for j in range(size):
                    r = np.sqrt((i - center)**2 + (j - center)**2)
                    theta = np.arctan2(j - center, i - center)
                    if abs(r - theta * 3) < 1.5:
                        img[i, j] = 1
                        
        elif pattern_type == 2:  # ç½‘çŠ¶æ¨¡å¼ (ç³»ç»Ÿæ€ç»´)
            for i in range(0, size, 8):
                img[i:i+2, :] = 1
                img[:, i:i+2] = 1
            # æ·»åŠ è¿æ¥ç‚¹
            for i in range(4, size, 8):
                for j in range(4, size, 8):
                    img[i-1:i+2, j-1:j+2] = 0.8
                    
        else:  # éšæœºæ¨¡å¼ (ç›´è§‰æ€ç»´)
            img = np.random.random((size, size))
            img = (img > 0.7).astype(float)
        
        # æ·»åŠ å™ªå£°
        noise = np.random.normal(0, 0.1, (size, size))
        img = np.clip(img + noise, 0, 1)
        
        return img
    
    # ç”Ÿæˆæ•°æ®é›†
    num_samples_per_class = 100
    images = []
    labels = []
    
    pattern_names = ['çº¿æ€§æ€ç»´', 'èºæ—‹æ€ç»´', 'ç½‘çŠ¶æ€ç»´', 'ç›´è§‰æ€ç»´']
    
    for pattern_type in range(4):
        print(f"ç”Ÿæˆ {pattern_names[pattern_type]} æ ·æœ¬...")
        for _ in range(num_samples_per_class):
            img = generate_pattern_image(pattern_type)
            images.append(img)
            labels.append(pattern_type)
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    images = np.array(images)
    labels = np.array(labels)
    
    print(f"æ•°æ®é›†å½¢çŠ¶: {images.shape}")
    print(f"æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
    print(f"ç±»åˆ«: {pattern_names}")
    
    return images, labels, pattern_names

def train_thinking_cnn():
    """è®­ç»ƒæ€ç»´æ¨¡å¼CNNåˆ†ç±»å™¨"""
    print("\nğŸ” è®­ç»ƒæ€ç»´æ¨¡å¼CNNåˆ†ç±»å™¨")
    print("-" * 40)
    
    # ç”Ÿæˆæ•°æ®
    images, labels, pattern_names = create_synthetic_image_data()
    
    # æ•°æ®é¢„å¤„ç†
    X = images.reshape(-1, 1, 32, 32)  # æ·»åŠ é€šé“ç»´åº¦
    y = labels
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_train = torch.FloatTensor(X_train)
    X_test = torch.FloatTensor(X_test)
    y_train = torch.LongTensor(y_train)
    y_test = torch.LongTensor(y_test)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(X_train, y_train)
    test_dataset = TensorDataset(X_test, y_test)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # åˆ›å»ºCNNæ¨¡å‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = ThinkingImageCNN(num_classes=4).to(device)
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # è®­ç»ƒæ¨¡å‹
    print("å¼€å§‹è®­ç»ƒCNN...")
    num_epochs = 50
    
    model.train()
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        correct = 0
        total = 0
        
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # ç»Ÿè®¡
            epoch_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
        
        if (epoch + 1) % 10 == 0:
            accuracy = 100 * correct / total
            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%')
    
    # æµ‹è¯•æ¨¡å‹
    print("æµ‹è¯•CNNæ¨¡å‹...")
    model.eval()
    with torch.no_grad():
        correct = 0
        total = 0
        
        for batch_X, batch_y in test_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            outputs = model(batch_X)
            _, predicted = torch.max(outputs, 1)
            
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
        
        test_accuracy = 100 * correct / total
        print(f'CNNæµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.2f}%')
    
    return model, pattern_names

# ==================== å¾ªç¯ç¥ç»ç½‘ç»œ (RNN) ====================

class ThinkingSequenceRNN(nn.Module):
    """ç”¨äºæ€ç»´åºåˆ—åˆ†æçš„RNN"""
    
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(ThinkingSequenceRNN, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTMå±‚
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=0.2)
        
        # å…¨è¿æ¥å±‚
        self.fc = nn.Linear(hidden_size, num_classes)
        
    def forward(self, x):
        # åˆå§‹åŒ–éšè—çŠ¶æ€
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        # LSTMå‰å‘ä¼ æ’­
        out, _ = self.lstm(x, (h0, c0))
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        out = self.fc(out[:, -1, :])
        
        return out

def create_thinking_sequence_data():
    """åˆ›å»ºæ€ç»´åºåˆ—æ•°æ®"""
    print("\nğŸ“ˆ åˆ›å»ºæ€ç»´åºåˆ—æ•°æ®é›†")
    print("-" * 40)
    
    # æ¨¡æ‹Ÿæ€ç»´è¿‡ç¨‹çš„æ—¶é—´åºåˆ—æ•°æ®
    np.random.seed(42)
    
    sequence_length = 20
    num_features = 5  # æ³¨æ„åŠ›ã€åˆ›é€ åŠ›ã€é€»è¾‘ã€è®°å¿†ã€æƒ…æ„Ÿ
    num_samples = 1000
    
    sequences = []
    labels = []
    
    for i in range(num_samples):
        # ç”Ÿæˆä¸åŒç±»å‹çš„æ€ç»´åºåˆ—
        sequence_type = i % 3
        
        if sequence_type == 0:  # å‘æ•£æ€ç»´æ¨¡å¼
            # åˆ›é€ åŠ›é€æ¸å¢å¼ºï¼Œæ³¨æ„åŠ›åˆ†æ•£
            base_pattern = np.array([0.3, 0.8, 0.4, 0.6, 0.7])
            sequence = []
            for t in range(sequence_length):
                noise = np.random.normal(0, 0.1, 5)
                creativity_boost = 0.3 * np.sin(t * 0.3)
                attention_decay = -0.2 * t / sequence_length
                
                point = base_pattern + noise
                point[1] += creativity_boost  # åˆ›é€ åŠ›
                point[0] += attention_decay   # æ³¨æ„åŠ›
                point = np.clip(point, 0, 1)
                sequence.append(point)
                
        elif sequence_type == 1:  # èšåˆæ€ç»´æ¨¡å¼
            # é€»è¾‘æ€§å¢å¼ºï¼Œæ³¨æ„åŠ›é›†ä¸­
            base_pattern = np.array([0.7, 0.4, 0.8, 0.6, 0.5])
            sequence = []
            for t in range(sequence_length):
                noise = np.random.normal(0, 0.1, 5)
                logic_boost = 0.2 * t / sequence_length
                attention_boost = 0.3 * np.cos(t * 0.2)
                
                point = base_pattern + noise
                point[2] += logic_boost      # é€»è¾‘
                point[0] += attention_boost  # æ³¨æ„åŠ›
                point = np.clip(point, 0, 1)
                sequence.append(point)
                
        else:  # å¹³è¡¡æ€ç»´æ¨¡å¼
            # å„é¡¹èƒ½åŠ›ä¿æŒå¹³è¡¡
            base_pattern = np.array([0.6, 0.6, 0.6, 0.6, 0.6])
            sequence = []
            for t in range(sequence_length):
                noise = np.random.normal(0, 0.05, 5)
                point = base_pattern + noise
                point = np.clip(point, 0, 1)
                sequence.append(point)
        
        sequences.append(sequence)
        labels.append(sequence_type)
    
    sequences = np.array(sequences)
    labels = np.array(labels)
    
    print(f"åºåˆ—æ•°æ®å½¢çŠ¶: {sequences.shape}")
    print(f"æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
    
    thinking_modes = ['å‘æ•£æ€ç»´', 'èšåˆæ€ç»´', 'å¹³è¡¡æ€ç»´']
    print(f"æ€ç»´æ¨¡å¼: {thinking_modes}")
    
    return sequences, labels, thinking_modes

def train_thinking_rnn():
    """è®­ç»ƒæ€ç»´åºåˆ—RNNåˆ†ç±»å™¨"""
    print("\nğŸ”„ è®­ç»ƒæ€ç»´åºåˆ—RNNåˆ†ç±»å™¨")
    print("-" * 40)
    
    # ç”Ÿæˆåºåˆ—æ•°æ®
    sequences, labels, thinking_modes = create_thinking_sequence_data()
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        sequences, labels, test_size=0.2, random_state=42, stratify=labels
    )
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_train = torch.FloatTensor(X_train)
    X_test = torch.FloatTensor(X_test)
    y_train = torch.LongTensor(y_train)
    y_test = torch.LongTensor(y_test)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(X_train, y_train)
    test_dataset = TensorDataset(X_test, y_test)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # åˆ›å»ºRNNæ¨¡å‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = ThinkingSequenceRNN(
        input_size=5, hidden_size=64, num_layers=2, num_classes=3
    ).to(device)
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # è®­ç»ƒæ¨¡å‹
    print("å¼€å§‹è®­ç»ƒRNN...")
    num_epochs = 30
    
    model.train()
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        correct = 0
        total = 0
        
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # ç»Ÿè®¡
            epoch_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
        
        if (epoch + 1) % 5 == 0:
            accuracy = 100 * correct / total
            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%')
    
    # æµ‹è¯•æ¨¡å‹
    print("æµ‹è¯•RNNæ¨¡å‹...")
    model.eval()
    with torch.no_grad():
        correct = 0
        total = 0
        
        for batch_X, batch_y in test_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            outputs = model(batch_X)
            _, predicted = torch.max(outputs, 1)
            
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
        
        test_accuracy = 100 * correct / total
        print(f'RNNæµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.2f}%')
    
    return model, thinking_modes

# ==================== ä¸»ç¨‹åº ====================

def main():
    """ä¸»ç¨‹åº"""
    print("ğŸ§  æ™ºèƒ½æ€ç»´é¡¹ç›® - ç¬¬å››å‘¨æ·±åº¦å­¦ä¹ åŸºç¡€")
    print("=" * 60)
    
    # 1. PyTorchåŸºç¡€æ¼”ç¤º
    device = pytorch_basics_demo()
    
    # 2. è®­ç»ƒç®€å•ç¥ç»ç½‘ç»œ
    print("\n" + "="*60)
    simple_model, scaler, label_encoder, losses, accuracies = train_thinking_classifier()
    
    # 3. è®­ç»ƒCNNæ¨¡å‹
    print("\n" + "="*60)
    cnn_model, pattern_names = train_thinking_cnn()
    
    # 4. è®­ç»ƒRNNæ¨¡å‹
    print("\n" + "="*60)
    rnn_model, thinking_modes = train_thinking_rnn()
    
    print("\nğŸ‰ ç¬¬å››å‘¨æ·±åº¦å­¦ä¹ åŸºç¡€å­¦ä¹ å®Œæˆï¼")
    print("âœ… å·²æŒæ¡æŠ€èƒ½:")
    print("  - PyTorchå¼ é‡æ“ä½œå’Œè‡ªåŠ¨æ¢¯åº¦")
    print("  - å…¨è¿æ¥ç¥ç»ç½‘ç»œ")
    print("  - å·ç§¯ç¥ç»ç½‘ç»œ (CNN)")
    print("  - å¾ªç¯ç¥ç»ç½‘ç»œ (RNN/LSTM)")
    print("  - æ·±åº¦å­¦ä¹ è®­ç»ƒæµç¨‹")
    
    print("\nğŸ“š ä¸‹ä¸€æ­¥: Webå‰ç«¯åŸºç¡€å­¦ä¹ ï¼ˆç¬¬7-8å‘¨ï¼‰")
    print("ğŸ’¡ å»ºè®®: å°è¯•ä¼˜åŒ–æ¨¡å‹ç»“æ„å’Œè¶…å‚æ•°ï¼")

# ==================== ç»ƒä¹ é¢˜ ====================

def practice_exercises():
    """ç¬¬å››å‘¨ç»ƒä¹ é¢˜"""
    print("\nğŸ¯ ç¬¬å››å‘¨ç»ƒä¹ é¢˜:")
    print("-" * 30)
    
    exercises = [
        "ç»ƒä¹ 1: å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„æ¿€æ´»å‡½æ•°",
        "ç»ƒä¹ 2: æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨æ¥ä¼˜åŒ–è®­ç»ƒ",
        "ç»ƒä¹ 3: å®ç°æ¨¡å‹çš„ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½",
        "ç»ƒä¹ 4: æ·»åŠ æ•°æ®å¢å¼ºæ¥æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›",
        "ç»ƒä¹ 5: å®ç°ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶å±‚",
        "ç»ƒä¹ 6: åˆ›å»ºä¸€ä¸ªå¤šä»»åŠ¡å­¦ä¹ ç½‘ç»œ",
        "ç»ƒä¹ 7: ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œè¿ç§»å­¦ä¹ "
    ]
    
    for exercise in exercises:
        print(exercise)
    
    print("\nğŸ’¡ æç¤º: è¿™äº›ç»ƒä¹ å°†å¸®åŠ©ä½ æ·±å…¥ç†è§£æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µï¼")

if __name__ == "__main__":
    main()
    practice_exercises() 